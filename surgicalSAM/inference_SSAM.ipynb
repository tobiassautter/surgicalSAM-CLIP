{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with clip based model of Surgical SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import Endovis18Dataset, Endovis17Dataset\n",
    "from model import Prototype_Prompt_Encoder, Learnable_Prototypes\n",
    "from model_forward import model_forward_function\n",
    "import argparse\n",
    "from utils import (\n",
    "    read_gt_endovis_masks,\n",
    "    create_binary_masks,\n",
    "    create_endovis_masks,\n",
    "    eval_endovis,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Process Arguments\n",
      "Namespace(dataset='endovis_2018', fold=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"======> Process Arguments\")\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--dataset\",\n",
    "    type=str,\n",
    "    default=\"endovis_2018\",\n",
    "    choices=[\"endovis_2018\", \"endovis_2017\"],\n",
    "    help=\"specify dataset\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fold\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    choices=[0, 1, 2, 3],\n",
    "    help=\"specify fold number for endovis_2017 dataset\",\n",
    ")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(\n",
    "    [\n",
    "        \"--dataset\",\n",
    "        \"endovis_2018\",\n",
    "        \"--fold\",\n",
    "        \"0\",\n",
    "    ]\n",
    ")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Set Parameters for Inference\n",
      "dataset_name: endovis_2018\n",
      "Dataset: endovis_2018\n",
      "data_root_dir:  ..\\data\\endovis_2018\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"======> Set Parameters for Inference\")\n",
    "dataset_name = args.dataset\n",
    "print(f\"dataset_name: {dataset_name}\")\n",
    "fold = args.fold\n",
    "thr = 0\n",
    "#data_root_dir = f\"../data/{dataset_name}\"\n",
    "data_root_dir = osp.join(\"..\", \"data\", dataset_name)\n",
    "# debug\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(\"data_root_dir: \", data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Load Dataset-Specific Parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"======> Load Dataset-Specific Parameters\")\n",
    "if \"18\" in dataset_name:\n",
    "    num_tokens = 2\n",
    "    dataset = Endovis18Dataset(data_root_dir=data_root_dir, mode=\"val\", vit_mode=\"h\")\n",
    "    surgicalSAM_ckp = f\"../ckp/surgical_sam/{dataset_name}/model_ckp.pth\"\n",
    "\n",
    "    gt_endovis_masks = read_gt_endovis_masks(data_root_dir=data_root_dir, mode=\"val\")\n",
    "\n",
    "elif \"17\" in dataset_name:\n",
    "    num_tokens = 4\n",
    "    dataset = Endovis17Dataset(\n",
    "        data_root_dir=data_root_dir, mode=\"val\", fold=fold, vit_mode=\"h\", version=0\n",
    "    )\n",
    "    surgicalSAM_ckp = f\"../ckp/surgical_sam/{dataset_name}/fold{fold}/model_ckp.pth\"\n",
    "\n",
    "    gt_endovis_masks = read_gt_endovis_masks(\n",
    "        data_root_dir=data_root_dir, mode=\"val\", fold=fold\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sam checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Load SAM\n",
      "checkpoint: ../ckp/sam/sam_vit_h_4b8939.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskDecoder(\n",
       "  (transformer): TwoWayTransformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TwoWayAttentionBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_token_to_image): Attention(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_image_to_token): Attention(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_attn_token_to_image): Attention(\n",
       "      (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (iou_token): Embedding(1, 256)\n",
       "  (mask_tokens): Embedding(4, 256)\n",
       "  (output_upscaling): Sequential(\n",
       "    (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): LayerNorm2d()\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (4): GELU(approximate='none')\n",
       "  )\n",
       "  (output_hypernetworks_mlps): ModuleList(\n",
       "    (0-3): 4 x MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (iou_prediction_head): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"======> Load SAM\")\n",
    "sam_checkpoint = \"../ckp/sam/sam_vit_h_4b8939.pth\"\n",
    "\n",
    "print(f\"checkpoint: {sam_checkpoint}\")\n",
    "# try loading the model\n",
    "try:\n",
    "    model_type = \"vit_h_no_image_encoder\"\n",
    "    sam_prompt_encoder, sam_decoder = sam_model_registry[model_type](\n",
    "        checkpoint=sam_checkpoint\n",
    "    )\n",
    "except KeyError:\n",
    "    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "sam_prompt_encoder.cuda()\n",
    "sam_decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Prototypes and Promt Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Load Prototypes and Prototype-based Prompt Encoder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"======> Load Prototypes and Prototype-based Prompt Encoder\")\n",
    "# define the models\n",
    "learnable_prototypes_model = Learnable_Prototypes(num_classes=7, feat_dim=256).cuda()\n",
    "protoype_prompt_encoder = Prototype_Prompt_Encoder(\n",
    "    feat_dim=256,\n",
    "    hidden_dim_dense=128,\n",
    "    hidden_dim_sparse=128,\n",
    "    size=64,\n",
    "    num_tokens=num_tokens,\n",
    ").cuda()\n",
    "\n",
    "# load the weight for prototype-based prompt encoder, mask decoder, and prototypes\n",
    "checkpoint = torch.load(surgicalSAM_ckp)\n",
    "protoype_prompt_encoder.load_state_dict(\n",
    "    checkpoint[\"prototype_prompt_encoder_state_dict\"]\n",
    ")\n",
    "sam_decoder.load_state_dict(checkpoint[\"sam_decoder_state_dict\"])\n",
    "learnable_prototypes_model.load_state_dict(checkpoint[\"prototypes_state_dict\"])\n",
    "\n",
    "# set requires_grad to False to the whole model\n",
    "for name, param in sam_prompt_encoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in sam_decoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in protoype_prompt_encoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in learnable_prototypes_model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Start Inference\n",
      "{'challengIoU': 80.329, 'IoU': 80.329, 'mcIoU': 58.868, 'mIoU': 77.946, 'cIoU_per_class': [83.656, 65.627, 58.742, 88.559, 21.224, 54.476, 39.792]}\n"
     ]
    }
   ],
   "source": [
    "print(\"======> Start Inference\")\n",
    "binary_masks = dict()\n",
    "protoype_prompt_encoder.eval()\n",
    "sam_decoder.eval()\n",
    "learnable_prototypes_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prototypes = learnable_prototypes_model()\n",
    "\n",
    "    for sam_feats, mask_names, cls_ids, _, _ in dataloader:\n",
    "\n",
    "        sam_feats = sam_feats.cuda()\n",
    "        cls_ids = cls_ids.cuda()\n",
    "\n",
    "        preds, preds_quality = model_forward_function(\n",
    "            protoype_prompt_encoder,\n",
    "            sam_prompt_encoder,\n",
    "            sam_decoder,\n",
    "            sam_feats,\n",
    "            prototypes,\n",
    "            cls_ids,\n",
    "        )\n",
    "\n",
    "        binary_masks = create_binary_masks(\n",
    "            binary_masks, preds, preds_quality, mask_names, thr\n",
    "        )\n",
    "\n",
    "endovis_masks = create_endovis_masks(binary_masks, 1024, 1280)\n",
    "endovis_results = eval_endovis(endovis_masks, gt_endovis_masks)\n",
    "\n",
    "print(endovis_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
